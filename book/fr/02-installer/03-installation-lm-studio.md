# üéØ 2.3 - Installation LM Studio NATIF

## üìã Ce que tu vas apprendre
- Comment installer LM Studio sur macOS
- Comment t√©l√©charger des mod√®les depuis l'interface graphique
- Comment configurer le serveur API local
- Comment connecter LM Studio √† OpenClaw

## üõ†Ô∏è Pr√©requis
- Chapitre 2.1 compl√©t√© (outils de base install√©s)
- Chapitre 2.2 compl√©t√© (Ollama install√© - optionnel mais recommand√©)
- Mac Studio M3 Ultra avec au moins 64 GB de RAM
- Environ 50 GB d'espace disque libre

## üìù √âtapes d√©taill√©es

### √âtape 1 : T√©l√©charger LM Studio

**Pourquoi ?** LM Studio offre une interface graphique facile pour g√©rer et tester des mod√®les d'IA. C'est parfait pour exp√©rimenter avant de choisir un mod√®le pour OpenClaw.

**Comment ?**
1. Ouvre Safari ou ton navigateur
2. Va sur : https://lmstudio.ai/
3. Clique sur "Download for Mac"
4. S√©lectionne "Apple Silicon" (pour M1/M2/M3)
5. Attends le t√©l√©chargement (environ 400 MB)

**V√©rification :**
Le fichier `LM-Studio-x.x.x-arm64.dmg` doit √™tre dans ton dossier T√©l√©chargements.

---

### √âtape 2 : Installer LM Studio

**Pourquoi ?** On va installer l'application dans le dossier Applications pour pouvoir la lancer facilement.

**Comment ?**
1. Double-clique sur le fichier `LM-Studio-x.x.x-arm64.dmg`
2. Une fen√™tre s'ouvre avec l'ic√¥ne LM Studio
3. Glisse l'ic√¥ne LM Studio vers le dossier Applications
4. Attends la copie (quelques secondes)
5. √âjecte le disque LM Studio (clic droit > √âjecter)

**Premier lancement :**
1. Ouvre le dossier Applications
2. Double-clique sur "LM Studio"
3. macOS peut te demander confirmation : clique sur "Ouvrir"
4. Accepte les conditions d'utilisation

**V√©rification :**
LM Studio s'ouvre avec un √©cran d'accueil.

---

### √âtape 3 : Configurer les param√®tres de base

**Pourquoi ?** On va configurer LM Studio pour utiliser au mieux le GPU M3 Ultra.

**Comment ?**
1. Dans LM Studio, clique sur l'ic√¥ne ‚öôÔ∏è (Param√®tres) en bas √† gauche
2. Va dans l'onglet "Runtime"
3. Configure ces param√®tres :

**Param√®tres GPU :**
- **GPU Acceleration :** Activ√© (ON)
- **GPU Layers :** Maximum (mettre 999 ou la valeur max propos√©e)
- **Metal :** Activ√© (ON)

**Param√®tres m√©moire :**
- **Context Length :** 8192 (ou 4096 si tu manques de RAM)
- **Max Tokens :** 2048

**Param√®tres serveur (onglet "Local Server") :**
- **Port :** 1234
- **CORS :** Activ√© (ON)
- **Verbose Logging :** D√©sactiv√© (OFF) pour de meilleures performances

4. Clique sur "Save" ou ferme la fen√™tre (les changements sont automatiques)

**V√©rification :**
Les param√®tres sont sauvegard√©s automatiquement.

---

### √âtape 4 : T√©l√©charger un mod√®le depuis l'interface

**Pourquoi ?** LM Studio a un moteur de recherche int√©gr√© pour trouver et t√©l√©charger des mod√®les facilement.

**Comment ?**
1. Clique sur l'ic√¥ne üîç (Discover) dans la barre lat√©rale gauche
2. Dans la barre de recherche, tape : `llama 3.1 8b`
3. Trouve un mod√®le avec le tag "Q4_K_M" ou "Q5_K_M" (bon √©quilibre taille/qualit√©)
4. Clique sur le bouton "Download" √† c√¥t√© du mod√®le

**Mod√®les recommand√©s pour commencer :**

| Mod√®le | Taille | Usage |
|--------|--------|-------|
| `TheBloke/Llama-2-7B-Chat-GGUF` | ~4 GB | Chat rapide |
| `lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF` | ~5 GB | Usage g√©n√©ral |
| `TheBloke/CodeLlama-13B-Instruct-GGUF` | ~7 GB | Programmation |
| `lmstudio-community/Meta-Llama-3.1-70B-Instruct-GGUF` | ~40 GB | Haute qualit√© |

**Pour t√©l√©charger Llama 3.1 8B :**
1. Recherche : `lmstudio-community Meta-Llama-3.1-8B-Instruct`
2. S√©lectionne la version `Q4_K_M` (environ 5 GB)
3. Clique sur "Download"
4. Attends la fin du t√©l√©chargement (5-20 minutes selon ta connexion)

**V√©rification :**
Le mod√®le appara√Æt dans la section "My Models" (ic√¥ne üìÅ).

---

### √âtape 5 : Charger et tester un mod√®le

**Pourquoi ?** On veut v√©rifier que le mod√®le fonctionne correctement avec le GPU.

**Comment ?**
1. Clique sur l'ic√¥ne üí¨ (Chat) dans la barre lat√©rale
2. En haut, clique sur "Select a model to load"
3. S√©lectionne le mod√®le que tu as t√©l√©charg√©
4. Attends le chargement (quelques secondes √† quelques minutes selon la taille)

**Premier test :**
1. Dans la zone de texte en bas, tape : `Bonjour, qui es-tu ?`
2. Appuie sur Entr√©e ou clique sur Envoyer
3. Attends la r√©ponse

**V√©rification :**
- Tu dois recevoir une r√©ponse en quelques secondes
- Regarde en bas de la fen√™tre : tu devrais voir "Metal" ou "GPU" indiqu√©

**Observer les performances :**
- En haut √† droite, tu vois la vitesse (tokens/seconde)
- Sur M3 Ultra, tu devrais avoir 30-100+ tokens/s selon le mod√®le

---

### √âtape 6 : D√©marrer le serveur API local

**Pourquoi ?** Pour qu'OpenClaw puisse utiliser LM Studio, on doit activer le serveur API qui √©coute sur le port 1234.

**Comment ?**
1. Clique sur l'ic√¥ne üîå (Local Server) dans la barre lat√©rale gauche
2. V√©rifie que le port est bien 1234
3. Clique sur "Start Server"
4. Le bouton devient vert et affiche "Server Running"

**Configuration CORS (important pour OpenClaw) :**
1. Dans les param√®tres du serveur, active "Enable CORS"
2. Cela permet √† OpenClaw d'acc√©der au serveur depuis le navigateur

**V√©rification en Terminal :**
```bash
curl -s http://localhost:1234/v1/models | jq .
```

**R√©sultat attendu :**
```json
{
  "object": "list",
  "data": [
    {
      "id": "lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF",
      "object": "model",
      "created": 1234567890,
      "owned_by": "lmstudio"
    }
  ]
}
```

---

### √âtape 7 : Tester l'API compatible OpenAI

**Pourquoi ?** LM Studio expose une API compatible avec OpenAI. OpenClaw peut l'utiliser comme s'il parlait √† GPT-4.

**Comment ?**
```bash
curl -s http://localhost:1234/v1/chat/completions -H "Content-Type: application/json" -d '{"model": "lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF", "messages": [{"role": "user", "content": "Dis bonjour en une phrase"}], "max_tokens": 50}' | jq -r '.choices[0].message.content'
```

**R√©sultat attendu :**
```
Bonjour ! Je suis ravi de vous rencontrer.
```

**Test avec stream (r√©ponse progressive) :**
```bash
curl -N http://localhost:1234/v1/chat/completions -H "Content-Type: application/json" -d '{"model": "lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF", "messages": [{"role": "user", "content": "Compte de 1 √† 5"}], "stream": true}'
```

---

### √âtape 8 : Configurer LM Studio pour d√©marrer avec le serveur

**Pourquoi ?** On veut que le serveur d√©marre automatiquement quand on ouvre LM Studio.

**Comment ?**
1. Clique sur ‚öôÔ∏è (Param√®tres)
2. Va dans l'onglet "Local Server"
3. Active "Start server on launch"
4. Active "Load last used model on launch"

**Configurer LM Studio au d√©marrage du Mac :**
1. Ouvre "Pr√©f√©rences Syst√®me"
2. Va dans "G√©n√©ral" > "Ouverture"
3. Clique sur "+"
4. S√©lectionne "LM Studio" dans Applications
5. Clique sur "Ajouter"

**V√©rification :**
Red√©marre LM Studio et v√©rifie que le serveur d√©marre automatiquement.

---

### √âtape 9 : Comparer les performances Ollama vs LM Studio

**Pourquoi ?** Les deux outils peuvent faire tourner les m√™mes mod√®les. On va voir lequel est le plus rapide pour ton usage.

**Script de comparaison :**
```bash
cat << 'EOF' > ~/openclaw/compare-backends.sh
#!/bin/bash
echo "=== Comparaison Ollama vs LM Studio ==="
echo ""

PROMPT="√âcris une liste de 5 fruits"

# Test Ollama
echo "1. Test Ollama (port 11434)..."
if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    START=$(python3 -c "import time; print(int(time.time()*1000))")
    curl -s http://localhost:11434/api/generate -d "{\"model\": \"llama3.2:3b\", \"prompt\": \"$PROMPT\", \"stream\": false}" > /dev/null
    END=$(python3 -c "import time; print(int(time.time()*1000))")
    echo "   ‚úÖ Ollama: $((END-START))ms"
else
    echo "   ‚ùå Ollama non disponible"
fi

# Test LM Studio
echo ""
echo "2. Test LM Studio (port 1234)..."
if curl -s http://localhost:1234/v1/models > /dev/null 2>&1; then
    MODEL=$(curl -s http://localhost:1234/v1/models | jq -r '.data[0].id')
    START=$(python3 -c "import time; print(int(time.time()*1000))")
    curl -s http://localhost:1234/v1/chat/completions -H "Content-Type: application/json" -d "{\"model\": \"$MODEL\", \"messages\": [{\"role\": \"user\", \"content\": \"$PROMPT\"}], \"max_tokens\": 100}" > /dev/null
    END=$(python3 -c "import time; print(int(time.time()*1000))")
    echo "   ‚úÖ LM Studio: $((END-START))ms"
else
    echo "   ‚ùå LM Studio non disponible"
fi

echo ""
echo "=== Comparaison termin√©e ==="
EOF
chmod +x ~/openclaw/compare-backends.sh
```

**Ex√©cuter la comparaison :**
```bash
~/openclaw/compare-backends.sh
```

---

### √âtape 10 : Configuration pour OpenClaw

**Pourquoi ?** On pr√©pare les param√®tres que OpenClaw utilisera pour se connecter √† LM Studio.

**Cr√©er le fichier de configuration :**
```bash
cat << 'EOF' > ~/openclaw/config/lm-studio.json
{
  "name": "LM Studio Local",
  "type": "openai-compatible",
  "baseURL": "http://localhost:1234/v1",
  "apiKey": "lm-studio",
  "models": {
    "default": "lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF",
    "code": "TheBloke/CodeLlama-13B-Instruct-GGUF",
    "fast": "TheBloke/Llama-2-7B-Chat-GGUF"
  },
  "settings": {
    "temperature": 0.7,
    "maxTokens": 2048,
    "contextLength": 8192
  }
}
EOF
```

**V√©rification :**
```bash
cat ~/openclaw/config/lm-studio.json | jq .
```

---

## ‚úÖ Checklist

Avant de passer au chapitre suivant, v√©rifie que :

- [ ] LM Studio est install√© dans Applications
- [ ] Au moins un mod√®le est t√©l√©charg√©
- [ ] Le mod√®le se charge et r√©pond dans le chat
- [ ] Le GPU Metal est utilis√© (visible en bas de la fen√™tre)
- [ ] Le serveur local est d√©marr√© sur le port 1234
- [ ] L'API r√©pond correctement (test curl)
- [ ] Le serveur est configur√© pour d√©marrer automatiquement
- [ ] Le fichier de configuration pour OpenClaw est cr√©√©

---

## ‚ö†Ô∏è D√©pannage

### LM Studio ne d√©marre pas
**Sympt√¥me :** L'application se ferme imm√©diatement
**Solution :**
```bash
rm -rf ~/Library/Application\ Support/LMStudio && rm -rf ~/Library/Caches/LMStudio
```
Puis relance LM Studio.

### Le mod√®le ne se charge pas
**Sympt√¥me :** Erreur "Failed to load model"
**Solutions :**
1. V√©rifie que tu as assez de RAM libre
2. Essaie un mod√®le plus petit (Q4 au lieu de Q8)
3. Ferme d'autres applications gourmandes

### Le serveur ne r√©pond pas sur le port 1234
**Sympt√¥me :** Connection refused
**Solutions :**
1. V√©rifie que le serveur est bien d√©marr√© (bouton vert)
2. V√©rifie qu'aucune autre application n'utilise le port :
```bash
lsof -i :1234
```
3. Change le port dans les param√®tres si n√©cessaire

### Performances lentes (moins de 10 tokens/seconde)
**Sympt√¥me :** G√©n√©ration tr√®s lente
**Solutions :**
1. V√©rifie que "Metal" est activ√© dans les param√®tres
2. Utilise un mod√®le quantifi√© (Q4_K_M ou Q5_K_M)
3. R√©duis le "Context Length" √† 4096
4. Ferme le navigateur et autres apps gourmandes

### Erreur "Out of memory"
**Sympt√¥me :** Crash ou erreur de m√©moire
**Solutions :**
1. Utilise un mod√®le plus petit
2. R√©duis "GPU Layers" √† 32 ou moins
3. Ferme d'autres applications
```bash
# Voir la m√©moire utilis√©e
top -l 1 | grep PhysMem
```

### Le mod√®le donne des r√©ponses incoh√©rentes
**Sympt√¥me :** R√©ponses bizarres ou r√©p√©titives
**Solutions :**
1. Ajuste la temp√©rature (0.7 est un bon d√©faut)
2. Essaie un mod√®le de meilleure qualit√© (Q5 au lieu de Q4)
3. V√©rifie que "repeat_penalty" est activ√©

---

## üîó Ressources

- [Site officiel LM Studio](https://lmstudio.ai/)
- [Documentation LM Studio](https://lmstudio.ai/docs)
- [Hugging Face Models](https://huggingface.co/models) - Source des mod√®les
- [TheBloke sur Hugging Face](https://huggingface.co/TheBloke) - Mod√®les quantifi√©s populaires
- [Guide des formats GGUF](https://github.com/ggerganov/llama.cpp/blob/master/gguf-py/README.md)

---

## üìä Comparaison Ollama vs LM Studio

| Aspect | Ollama | LM Studio |
|--------|--------|-----------|
| **Interface** | Terminal | Graphique |
| **API** | Format propre + OpenAI | Format OpenAI |
| **Port par d√©faut** | 11434 | 1234 |
| **Gestion mod√®les** | `ollama pull` | Interface graphique |
| **Personnalisation** | Modelfiles | Param√®tres GUI |
| **Usage m√©moire** | L√©g√®rement plus bas | L√©g√®rement plus haut |
| **Id√©al pour** | Production, scripts | Tests, exp√©rimentation |

**Recommandation pour OpenClaw :**
- Utilise **Ollama** pour la production (plus stable, plus l√©ger)
- Utilise **LM Studio** pour tester de nouveaux mod√®les avant de les adopter

---

## ‚û°Ô∏è Prochaine √©tape

LM Studio est install√© et configur√© ! Tu as maintenant deux backends d'IA locaux qui fonctionnent. Dans le prochain chapitre, on va installer **k3s**, la version l√©g√®re de Kubernetes qui va orchestrer tous nos conteneurs.

**Chapitre suivant :** [2.4 - Installation k3s sur macOS](./04-installation-k3s.md)
