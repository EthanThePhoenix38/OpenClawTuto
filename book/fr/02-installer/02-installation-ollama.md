# üéØ 2.2 - Installation Ollama NATIF avec config GPU M3

## üìã Ce que tu vas apprendre
- Comment installer Ollama nativement sur macOS (pas dans Docker)
- Comment configurer Ollama pour utiliser tout le GPU du M3 Ultra
- Comment t√©l√©charger et tester ton premier mod√®le IA
- Comment v√©rifier que le GPU est bien utilis√©

## üõ†Ô∏è Pr√©requis
- Chapitre 2.1 compl√©t√© (tous les outils install√©s)
- Mac Studio M3 Ultra avec au moins 64 GB de RAM
- Connexion Internet (pour t√©l√©charger les mod√®les)

## üìù √âtapes d√©taill√©es

### √âtape 1 : T√©l√©charger et installer Ollama

**Pourquoi ?** Ollama est le serveur qui va faire tourner les mod√®les d'IA. On l'installe en NATIF (pas dans Docker) pour qu'il puisse utiliser directement le GPU Metal du M3 Ultra.

**Comment ?**
1. Ouvre Safari ou ton navigateur
2. Va sur : https://ollama.ai/download
3. Clique sur "Download for macOS"
4. Ouvre le fichier Ollama-darwin.zip t√©l√©charg√©
5. Glisse l'application Ollama dans le dossier Applications
6. Ouvre Ollama depuis le dossier Applications
7. Clique sur "Ouvrir" si macOS te demande confirmation

**Alternative avec Homebrew :**
```bash
brew install ollama
```

**V√©rification :**
```bash
ollama --version
```

**R√©sultat attendu :**
```
ollama version 0.x.x
```

---

### √âtape 2 : D√©marrer le serveur Ollama

**Pourquoi ?** Le serveur Ollama doit tourner en arri√®re-plan pour r√©pondre aux requ√™tes d'Phoenix.

**Comment (GUI) :**
1. Ouvre l'application Ollama depuis Applications
2. Une ic√¥ne de lama appara√Æt dans la barre de menu en haut
3. Ollama d√©marre automatiquement son serveur

**Comment (Terminal) :**
```bash
ollama serve &
```

**V√©rification :**
```bash
curl -s http://localhost:11434/api/tags | jq .
```

**R√©sultat attendu :**
```json
{
  "models": []
}
```

C'est normal que la liste soit vide, on n'a pas encore t√©l√©charg√© de mod√®le !

---

### √âtape 3 : Configurer Ollama pour le M3 Ultra

**Pourquoi ?** Par d√©faut, Ollama fonctionne bien, mais on peut l'optimiser pour mieux utiliser les 24 coeurs du M3 Ultra.

**Comment ?**
1. Cr√©e le fichier de configuration :

```bash
mkdir -p ~/.ollama && cat << 'EOF' > ~/.ollama/config.json
{
  "gpu": true,
  "num_gpu": 999,
  "num_thread": 16,
  "num_ctx": 8192,
  "num_batch": 512
}
EOF
```

**Explication des param√®tres :**
- `gpu: true` : Active l'utilisation du GPU Metal
- `num_gpu: 999` : Utilise tous les coeurs GPU disponibles
- `num_thread: 16` : Utilise 16 threads CPU (laisse 8 pour le syst√®me)
- `num_ctx: 8192` : Taille du contexte (m√©moire de conversation)
- `num_batch: 512` : Taille des lots pour le traitement

**Configuration des variables d'environnement :**
```bash
cat << 'EOF' >> ~/.zprofile
# Configuration Ollama pour M3 Ultra
export OLLAMA_HOST="127.0.0.1:11434"
export OLLAMA_KEEP_ALIVE="24h"
export OLLAMA_NUM_PARALLEL="4"
export OLLAMA_MAX_LOADED_MODELS="3"
export OLLAMA_FLASH_ATTENTION="1"
EOF
source ~/.zprofile
```

**Explication :**
- `OLLAMA_HOST` : Adresse d'√©coute (localhost seulement pour la s√©curit√©)
- `OLLAMA_KEEP_ALIVE` : Garde les mod√®les en m√©moire 24h
- `OLLAMA_NUM_PARALLEL` : Permet 4 requ√™tes simultan√©es
- `OLLAMA_MAX_LOADED_MODELS` : Maximum 3 mod√®les en m√©moire
- `OLLAMA_FLASH_ATTENTION` : Active l'attention flash (plus rapide)

**Red√©marrer Ollama pour appliquer :**
1. Clique sur l'ic√¥ne Ollama dans la barre de menu
2. Clique sur "Quit Ollama"
3. R√©ouvre Ollama depuis Applications

**V√©rification :**
```bash
echo "OLLAMA_HOST: $OLLAMA_HOST" && echo "OLLAMA_KEEP_ALIVE: $OLLAMA_KEEP_ALIVE"
```

---

### √âtape 4 : T√©l√©charger un mod√®le de test

**Pourquoi ?** On va t√©l√©charger un petit mod√®le pour v√©rifier que tout fonctionne avant d'installer les gros.

**Comment ?**
```bash
ollama pull llama3.2:3b
```

**Ce qui va se passer :**
- T√©l√©chargement d'environ 2 GB
- Extraction automatique
- Le mod√®le sera pr√™t √† l'emploi

**Temps estim√© :** 1-5 minutes selon ta connexion.

**V√©rification :**
```bash
ollama list
```

**R√©sultat attendu :**
```
NAME            ID              SIZE    MODIFIED
llama3.2:3b     a80c4f17acd5    2.0 GB  Just now
```

---

### √âtape 5 : Tester le mod√®le

**Pourquoi ?** On v√©rifie que le mod√®le fonctionne et utilise bien le GPU.

**Comment ?**
```bash
ollama run llama3.2:3b "Dis bonjour en une phrase"
```

**R√©sultat attendu :**
Une r√©ponse en quelques secondes, par exemple :
```
Bonjour ! Je suis ravi de vous rencontrer.
```

**Test via l'API (comme Phoenix va l'utiliser) :**
```bash
curl -s http://localhost:11434/api/generate -d '{"model": "llama3.2:3b", "prompt": "R√©ponds en un mot: 2+2=", "stream": false}' | jq -r '.response'
```

**R√©sultat attendu :**
```
4
```

---

### √âtape 6 : V√©rifier l'utilisation du GPU

**Pourquoi ?** On veut √™tre s√ªr que le M3 Ultra travaille avec son GPU, pas juste le CPU.

**Comment ?**
1. Ouvre "Moniteur d'activit√©" (Cmd + Espace, tape "Moniteur", Entr√©e)
2. Clique sur l'onglet "GPU"
3. Pendant qu'Ollama g√©n√®re du texte, tu dois voir de l'activit√©

**Test en ligne de commande :**
```bash
sudo powermetrics --samplers gpu_power -i 1000 -n 1 2>/dev/null | grep -E "GPU|Power"
```

**Pendant un test avec Ollama (dans un autre terminal) :**
```bash
ollama run llama3.2:3b "√âcris un po√®me de 10 lignes sur la technologie"
```

**Ce que tu dois voir :**
L'utilisation GPU doit augmenter pendant la g√©n√©ration.

---

### √âtape 7 : T√©l√©charger les mod√®les recommand√©s pour Phoenix

**Pourquoi ?** Phoenix fonctionne mieux avec certains mod√®les. On va t√©l√©charger ceux recommand√©s.

**Mod√®le principal - Llama 3.1 70B (n√©cessite 64+ GB RAM) :**
```bash
ollama pull llama3.1:70b
```
**Taille :** ~40 GB - **Temps :** 20-60 minutes

**Mod√®le alternatif - Llama 3.1 8B (pour moins de RAM) :**
```bash
ollama pull llama3.1:8b
```
**Taille :** ~4.7 GB - **Temps :** 2-10 minutes

**Mod√®le de code - CodeLlama 34B :**
```bash
ollama pull codellama:34b
```
**Taille :** ~19 GB - **Temps :** 10-30 minutes

**Mod√®le pour embeddings :**
```bash
ollama pull nomic-embed-text
```
**Taille :** ~274 MB - **Temps :** 1 minute

**V√©rification de tous les mod√®les :**
```bash
ollama list
```

**R√©sultat attendu :**
```
NAME                ID              SIZE      MODIFIED
llama3.1:70b        xxxxx           40 GB     x minutes ago
llama3.1:8b         xxxxx           4.7 GB    x minutes ago
codellama:34b       xxxxx           19 GB     x minutes ago
nomic-embed-text    xxxxx           274 MB    x minutes ago
llama3.2:3b         xxxxx           2.0 GB    x minutes ago
```

---

### √âtape 8 : Cr√©er un Modelfile personnalis√© pour Phoenix

**Pourquoi ?** On peut cr√©er un mod√®le personnalis√© avec des param√®tres optimis√©s pour Phoenix.

**Comment ?**
```bash
cat << 'EOF' > ~/phoenix/config/Modelfile.phoenix
FROM llama3.1:8b

# Param√®tres optimis√©s pour Phoenix
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER num_ctx 8192
PARAMETER repeat_penalty 1.1

# Prompt syst√®me pour Phoenix
SYSTEM """Tu es un assistant IA int√©gr√© √† Phoenix, une plateforme de d√©veloppement s√©curis√©e. Tu aides les d√©veloppeurs avec leur code, leurs questions techniques et leurs projets. Tu r√©ponds toujours en fran√ßais quand on te parle en fran√ßais. Tu es pr√©cis, concis et professionnel."""
EOF
```

**Cr√©er le mod√®le personnalis√© :**
```bash
ollama create phoenix-assistant -f ~/phoenix/config/Modelfile.phoenix
```

**V√©rification :**
```bash
ollama list | grep phoenix
```

**R√©sultat attendu :**
```
phoenix-assistant    xxxxx    4.7 GB    Just now
```

**Tester le mod√®le personnalis√© :**
```bash
ollama run phoenix-assistant "Qui es-tu ?"
```

---

### √âtape 9 : Configurer Ollama pour d√©marrer automatiquement

**Pourquoi ?** On veut qu'Ollama d√©marre tout seul quand le Mac s'allume.

**Comment (GUI) :**
1. Ouvre "Pr√©f√©rences Syst√®me" > "G√©n√©ral" > "Ouverture"
2. Clique sur "+"
3. S√©lectionne "Ollama" dans Applications
4. Clique sur "Ajouter"

**Comment (Terminal) :**
```bash
cat << 'EOF' > ~/Library/LaunchAgents/com.ollama.server.plist
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
    <key>Label</key>
    <string>com.ollama.server</string>
    <key>ProgramArguments</key>
    <array>
        <string>/usr/local/bin/ollama</string>
        <string>serve</string>
    </array>
    <key>RunAtLoad</key>
    <true/>
    <key>KeepAlive</key>
    <true/>
    <key>StandardOutPath</key>
    <string>/tmp/ollama.log</string>
    <key>StandardErrorPath</key>
    <string>/tmp/ollama.error.log</string>
</dict>
</plist>
EOF
launchctl load ~/Library/LaunchAgents/com.ollama.server.plist
```

**V√©rification :**
```bash
launchctl list | grep ollama
```

---

### √âtape 10 : Script de test complet

**Pourquoi ?** Un script qui teste tout d'un coup pour √™tre s√ªr que tout marche.

**Comment ?**
```bash
cat << 'EOF' > ~/phoenix/test-ollama.sh
#!/bin/bash
echo "=== Test Ollama pour Phoenix ==="
echo ""

# Test 1: Serveur actif
echo "1. V√©rification du serveur..."
if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo "   ‚úÖ Serveur Ollama actif sur le port 11434"
else
    echo "   ‚ùå Serveur Ollama non accessible"
    exit 1
fi

# Test 2: Mod√®les install√©s
echo ""
echo "2. Mod√®les install√©s :"
ollama list | while read line; do echo "   $line"; done

# Test 3: Test de g√©n√©ration
echo ""
echo "3. Test de g√©n√©ration rapide..."
RESPONSE=$(curl -s http://localhost:11434/api/generate -d '{"model": "llama3.2:3b", "prompt": "R√©ponds seulement OK", "stream": false}' | jq -r '.response' 2>/dev/null)
if [[ "$RESPONSE" == *"OK"* ]] || [[ -n "$RESPONSE" ]]; then
    echo "   ‚úÖ G√©n√©ration fonctionne"
    echo "   R√©ponse: $RESPONSE"
else
    echo "   ‚ùå Probl√®me de g√©n√©ration"
fi

# Test 4: Temps de r√©ponse
echo ""
echo "4. Test de performance..."
START=$(date +%s%N)
curl -s http://localhost:11434/api/generate -d '{"model": "llama3.2:3b", "prompt": "1+1=", "stream": false}' > /dev/null
END=$(date +%s%N)
DURATION=$(( (END - START) / 1000000 ))
echo "   Temps de r√©ponse: ${DURATION}ms"

echo ""
echo "=== Tests termin√©s ==="
EOF
chmod +x ~/phoenix/test-ollama.sh
```

**Ex√©cuter les tests :**
```bash
~/phoenix/test-ollama.sh
```

**R√©sultat attendu :**
```
=== Test Ollama pour Phoenix ===

1. V√©rification du serveur...
   ‚úÖ Serveur Ollama actif sur le port 11434

2. Mod√®les install√©s :
   NAME                ID              SIZE      MODIFIED
   llama3.1:8b         xxxxx           4.7 GB    x minutes ago
   llama3.2:3b         xxxxx           2.0 GB    x minutes ago

3. Test de g√©n√©ration rapide...
   ‚úÖ G√©n√©ration fonctionne
   R√©ponse: OK

4. Test de performance...
   Temps de r√©ponse: 1234ms

=== Tests termin√©s ===
```

---

## ‚úÖ Checklist

Avant de passer au chapitre suivant, v√©rifie que :

- [ ] Ollama est install√© (version 0.x.x ou plus)
- [ ] Le serveur Ollama tourne sur le port 11434
- [ ] Les variables d'environnement sont configur√©es
- [ ] Au moins un mod√®le est t√©l√©charg√© (llama3.2:3b minimum)
- [ ] Le test de g√©n√©ration fonctionne
- [ ] Le GPU est utilis√© pendant la g√©n√©ration
- [ ] Ollama est configur√© pour d√©marrer automatiquement
- [ ] Le script de test passe sans erreur

---

## ‚ö†Ô∏è D√©pannage

### Ollama ne d√©marre pas
**Sympt√¥me :** L'application se ferme imm√©diatement
**Solution :**
```bash
rm -rf ~/.ollama/logs && rm -rf ~/.ollama/*.pid && open -a Ollama
```

### Le port 11434 est d√©j√† utilis√©
**Sympt√¥me :** "address already in use"
**Solution :**
```bash
lsof -i :11434 | grep LISTEN | awk '{print $2}' | xargs kill -9
```

### Le mod√®le ne se t√©l√©charge pas
**Sympt√¥me :** Erreur de connexion ou timeout
**Solution :**
```bash
export OLLAMA_ORIGINS="*" && ollama pull llama3.2:3b
```

### La g√©n√©ration est tr√®s lente (>30 secondes)
**Sympt√¥me :** Le mod√®le met trop de temps √† r√©pondre
**Solution :** V√©rifie que le GPU est utilis√© :
```bash
sudo powermetrics --samplers gpu_power -i 1000 -n 3
```
Si le GPU n'est pas actif, red√©marre Ollama.

### "Not enough memory"
**Sympt√¥me :** Erreur de m√©moire insuffisante
**Solution :** Utilise un mod√®le plus petit ou ferme d'autres applications :
```bash
ollama stop llama3.1:70b && ollama run llama3.1:8b "test"
```

### Les mod√®les disparaissent apr√®s red√©marrage
**Sympt√¥me :** `ollama list` est vide
**Solution :** Les mod√®les sont stock√©s dans ~/.ollama/models. V√©rifie :
```bash
ls -la ~/.ollama/models/
```

---

## üîó Ressources

- [Documentation officielle Ollama](https://ollama.ai/docs)
- [Liste des mod√®les Ollama](https://ollama.ai/library)
- [API Ollama](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Optimisation GPU Apple Silicon](https://github.com/ollama/ollama/blob/main/docs/gpu.md)
- [Modelfile Reference](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)

---

## ‚û°Ô∏è Prochaine √©tape

Ollama est install√© et optimis√© pour ton M3 Ultra ! Dans le prochain chapitre, on va installer **LM Studio**, une interface graphique qui permet de tester et comparer facilement diff√©rents mod√®les.

**Chapitre suivant :** [2.3 - Installation LM Studio NATIF](./03-installation-lm-studio.md)
